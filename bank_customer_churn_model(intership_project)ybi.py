# -*- coding: utf-8 -*-
"""BANK CUSTOMER CHURN MODEL(INTERSHIP PROJECT)YBI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18pGtnmqVwN6-Fyr1gCDI2s6hNdrC0rPw

BANK CUSTOMER CHURN MODEL

LEARNING ONJECTIVE
1.DATA ENCODING
2.FEATURE SCALING
3.HANDLING IMBALANCE DATA
  a. RANDOM UNDERSAMPLE DATA
  b. RANDOM OVERSAMPLE DATA
4.SUPPORT VECTOR MACHINE CLASSIFIER
5.GRID SEARCH FOR HYPERTUNNING

IMPORT LIBRARY
"""

import pandas as pd

import seaborn as sns

import numpy as np

import matplotlib.pyplot as plt

"""IMPORT DATA"""

df=pd.read_csv(' https://github.com/YBI-Foundation/Dataset/raw/main/Bank%20Churn%20Modelling.csv')

"""DESCRIBE DATA"""

df.head()

df.info()

df.describe()

df.shape

df.columns

df=df.set_index('CustomerId')    #seting customerid as index

df.info()

"""                                                                         
                                                                          
                                                                       ENCODING
"""

df['Geography'].value_counts

df.replace({'Geography':{'france' : 2,'Germany': 1,'Spain':0}}, inplace =True)    #we r replacing values for france by 2 .....

df['Gender'].value_counts

df.replace({'Gender':{'Female':1,'Male':0}} , inplace=True)

df['Num Of Products'].value_counts()

df.replace({'Num Of Products':{1:0,2:1,3:1,4:1}}, inplace=True)   #for value 3 and 4 size eis less so if we do this, then sample size wil nor=t be  enough

df['Has Credit Card'].value_counts()

df['Is Active Member'].value_counts()

df['Churn'].value_counts()

df.loc[(df['Balance']==0), 'Churn'].value_counts()  #place where balance is o and named as churn             500 leave bnk have 0 balance and 3117 don't leave also having 0 balance

df['Zero Balance']=np.where(df['Balance']>  0,1,0)    #create new column zero balance  ehre balance names as 1 or 0

df['Zero Balance'].hist()   #histogram of zero balace

df.groupby(['Geography','Churn']).value_counts()

"""                                                                        
                                                                      MODELLING
                                                                      (label and features)
"""

x=df.drop(['Surname','Churn'], axis=1)

y=df['Churn']

x.shape

y.shape

"""HANDLING IMBALANCE DATA

in this we used to work on problem like 1.Disease detection
                                        2.fraud detection
                                        3.spam filtering
                                        4.churn detection

IN THIS WE USED TO WORK WHERE N0.OUTPUT IS LOW ,for ex :there are low no. of people who leavi bank or low no.of people suffer from disease.



two technique to resolve 1.UNDERSAMPLING(MEANS CONVERT ORIGINAL DATA TO SAME AS NO. OF MINORITY CLASS)

                         2.OVERSAMLING(CONVERT MINORITY DATA TO NO. OF MAJORITY CLASS)

"""

df['Churn'].value_counts()

sns.countplot(x='Churn', data=df);

"""        RANDOM UNDERSAMPLING"""

from imblearn.under_sampling import RandomUnderSampler

rus=RandomUnderSampler(random_state=2529)

x_rus,y_rus =rus.fit_resample(x,y)

x.shape, y.shape , x_rus.shape, y_rus.shape

x.value_counts() , y.value_counts()

y_rus.value_counts()

y_rus.plot(kind='hist')

"""random oversampling"""

from imblearn.over_sampling import RandomOverSampler

ros=RandomOverSampler(random_state=2529)

x_ros,y_ros=ros.fit_resample(x,y)

x_ros.shape,y_ros.shape,x.shape,y.shape

y.value_counts()

y_ros.value_counts()

y_ros.plot(kind='hist')

"""train test and split"""

from sklearn.model_selection import train_test_split

x_train,x_test, y_train,y_test= train_test_split(x,y,random_state=2529)   #split data

x_train_rus,x_test_rus,y_train_rus, y_test_rus=train_test_split(x_rus,y_rus)   #split undersampling data

x_train_ros,x_test_ros,y_train_ros, y_test_ros=train_test_split(x_ros,y_ros)    #split oversampling data

x_train.shape,x_test.shape,y_train.shape,y_test.shape

"""STANDARDIZE FEATURES"""

from sklearn.preprocessing import StandardScaler

sc=StandardScaler()

"""STANDADIZE ORIGINAL DATA"""

x_train[['CreditScore','Age','Tenure','Balance','Estimated Salary']]=sc.fit_transform(x_train[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

x_test[['CreditScore','Age','Tenure','Balance','Estimated Salary']]=sc.fit_transform(x_test[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

"""STANDARDIZE RANDOM UPERSAMPLING DATA"""

x_train_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']]=sc.fit_transform(x_train_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

x_test_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']]=sc.fit_transform(x_test_rus[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

"""STANDARDIZE RANDOM OVERSAMPLING DATA"""

x_train_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']]=sc.fit_transform(x_train_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

x_test_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']]=sc.fit_transform(x_test_ros[['CreditScore','Age','Tenure','Balance','Estimated Salary']])

"""SUPPORT VECTOR MACHINE CLASSIFIER"""

from sklearn.svm  import  SVC

svc=SVC()

svc.fit(x_train,y_train)

y_pred=svc.predict(x_test)

y_pred

"""MODEL ACCURACY"""

from sklearn.metrics import confusion_matrix, classification_report

confusion_matrix(y_test,y_pred)

print (classification_report(y_test,y_pred))

"""HYPERPARAMETER TUNINGG"""

from sklearn.model_selection import GridSearchCV

param_grid={'C':[0.1,1,10],
            'gamma':[1,0.1,0.01],
            'kernel':['rbf'],
            'class_weight':['Balanced']}

grid=GridSearchCV(SVC(),param_grid,refit=True,verbose=2,cv=2)

grid.fit(x_train,y_train)

print(grid.best_estimator_)

grid_predictions=grid.predict(x_test)

confusion_matrix(y_test,grid_predictions)

print(classification_report(y_test,grid_predictions))

"""MODEL WITH RANDOM UNDER SMAPLING"""

svc_rus=SVC()

svc.fit(x_train_rus,y_train_rus)

y_pred_rus=svc_rus.fit(x_test_rus)

"""MODEL ACCURACY"""

confusion_matrix(y_test_rus,y_pred_rus)

print(classification_report(y_test_rus,y_pred_rus))

"""hyperparameter tunning

"""

param_grid={'C':[0.1,1,10],
            'gamma':[1,0.1,0.01],
            'kernel':['rbf'],
            'class_weight':['Balanced']}

grid=GridSearchCV(SVC(),param_grid,refit=True,verbose=2,cv=2)

grid.fit(x_train_rus,y_train_rus)

print(grid_rus.best_estimator_)

grid_rus_predictions=grid_rus.predict(x_testr_rus)

confusion_matrix(y_test_rus,grid_predictions_rus)

print(classification_report(y_test_rus,grid_predictions_rus))

"""MODEL RANDOM OVERSAMPLIN

"""

svc_ros=SVC()

svc_ros.fit(x_tarin_ros,y_train_ros)

y_pred_ros=svc_ros.fit(x_test_ros)

"""MODEL ACCURACY"""

confusion_matrix(y_test_ros,y_pred_ros)

print(classification_report(y_test_ros,y_pred_ros))

param_grid={'C':[0.1,1,10],
            'gamma':[1,0.1,0.01],
            'kernel':['rbf'],
            'class_weight':['Balanced']}

grid=GridSearchCV(SVC(),param_grid,refit=True,verbose=2,cv=2)

grid.fit(x_train_ros,y_train_ros)

print(grid_ros.best_estimator_)

grid_ros_predictions=grid_rus.predict(x_testr_ros)

confusion_matrix(y_test_ros,grid_predictions_ros)

print(classification_report(y_test_ros,grid_predictions_ros))

